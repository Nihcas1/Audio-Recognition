# Audio-Recognition
The framework for audio-based emotion detection comprises two distinct phases:
Phase 1: 
  Development and Evaluation of the Audio Classification Model using Convolutional Neural Networks (CNN)
In this initial phase, the focus lies on creating a robust audio classification model leveraging Convolutional Neural Networks (CNN). The primary objectives involve training the model to accurately classify audio data based on emotional cues and thoroughly evaluating its performance. Metrics such as accuracy, precision, recall, and F1 score will be considered to gauge the effectiveness of the CNN-based model in discerning various emotional states.
Phase 2:
  Implementation of an Interactive Web Interface using Streamlit for Emotion Detection
Following the successful development and evaluation of the audio classification model, the next phase involves creating an accessible and user-friendly interface. This is achieved through the integration of Streamlit, a framework designed for streamlined web application development. The web page facilitates an intuitive and easy interaction for users seeking to detect emotions within audio files. Users can upload their audio samples, and the web page, powered by the trained CNN model, provides real-time emotion predictions. This dynamic and responsive interface enhances the overall user experience and extends the applicability of the emotion detection model.
